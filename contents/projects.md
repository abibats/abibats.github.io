### 基于Transformer的加法计算模型项目介绍

#### 项目描述
- 本项目是一个基于Transformer架构的序列到序列（seq2seq）模型，通过多头自注意力机制（Multi-Head Self-Attention），旨在解决字符串形式的加法运算问题。输入为两个数字字符串（用字母a分隔），输出为它们的数值和对应的字符串。例如，输入"123a456"，模型应输出"579"。通过深度学习技术，模型能够自动学习输入与输出之间的复杂映射关系，最终实现高精度的加法计算。
#### 创新点
- 轻量化设计：通过精简词表（仅39类）和模型维度（32维），显著降低计算复杂度。
- 动态数据生成：训练时实时生成无限量随机加法问题，避免过拟合，增强鲁棒性。
- 相比于贪心搜索（Greedy Search）（即每次都选概率最高的词），Beam Search 可以同时保留多个候选序列，从而避免搜索局部最优解。
#### 成果
- 训练准确率：经过10轮训练，模型在验证集上的准确率可达99%，损失值稳定收敛至接近0。
- 泛化能力：模型能够正确处理不同长度的数字字符串（如10~20位加法），并输出精确结果