[//]: # (### 基于Llama 2微调实现聊天机器人)
### <div align="center">基于 Llama 2 微调实现的聊天机器人</div>
#### 项目描述：
- 本项目通过Llama2微调，训练chat数据集，从而实现一个日常聊天机器人。
#### 关键实现：

- 多头注意力机制（Multi-Head Attention）
- 前馈神经网络（FeedForward Network）

#### 成果： 
- 通过多次训练以后，该模型能够基本满足日常交流。


[//]: # (### 基于Transformer的加法计算模型项目)
### <div align="center">基于Transformer的加法计算模型项目</div>
#### 项目描述：
- 本项目是一个基于Transformer架构的序列到序列（seq2seq）模型，通过多头自注意力机制（Multi-Head Self-Attention），手写transformer模型，模型能够自动学习输入与输出之间的复杂映射关系，最终实现高精度的加法计算。
#### 成果：
- 通过10轮训练，模型在验证集上的准确率高达99%，损失值稳定收敛至接近0.

[//]: # (### 基于CNN的手写数字识别)

### <div align="center">基于CNN的手写数字识别</div>

#### 项目描述：
- 本项目实现了一个卷积神经网络（CNN）模型，能够用于手写数字体的识别。
#### 关键实现：
- 卷积层、池化层、激活函数、全连接层。
#### 成果：
- 相较于感知机（MLP），卷积神经网络收敛速度更快，预测的准确率更高，可达99%。